---
layout: post
title:  "Deep Learning Part 2 - Restricted Boltzmann Machines and Feedforward Neural Networks"
date:   2018-01-11 21:45:00 -0800
categories: blog dl tutorial
---
<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

This is the second in a several-part series on the basics of deep learning, presented in an easy-to-read, lightweight format. [Here]({% post_url 2017-12-30-dl-part-1 %}) is a link to the first one. Previous experience with basic probability and matrix algebra will be helpful, but not required. Send any comments or corrections to [josh@jzhanson.com](mailto:josh@jzhanson.com).

Mathematically, Restricted Boltzmann Machines are derived from the Maxwell-Boltzmann Distribution plus matrix algebra, which we'll go over in this post. We'll also use that as a bridge to connect to the basics of neural networks.

## The Boltzmann Distribution

Let us first define **x** to be a vector of *n* outcomes, where each *x<sub>i</sub>* can either be 0 or 1. Of course, each *x<sub>i</sub>* can have a different probability of being 1. The probabilities can even be conditional, *a la* Markov Chains. But more on that later. In the previous post, we have usually thought of *x* as being a single random variable. Here, however, it is a vector of individual random variables.

$$
  \textbf{x} = \begin{bmatrix} x_1  &  x_2  &  \ldots  &  x_n \end{bmatrix}, \: x_i \in \{0, 1\}
$$

With that definition out of the way, we can examine the Boltzmann distribution, invented by Ludwig Boltzmann, which models a bunch of things in physics, like how a hot object cools, or how energy dissipates into the environment. We have

$$
  p(x) = \frac{1}{Z} \exp (-E(\textbf{x})), \: E(\textbf{x}) = \textbf{x}^T \textbf{U} \textbf{x} + \textbf{b}^T \textbf{x}
$$

Here, *Z* is the partition function or normalizing constant which makes sure that the distribution integrates to one. It has actually been proven that the partition function *Z* is intractable, which means that it cannot be efficiently solved or evaluated. The exp function is the same as raising the constant *e* to the function argument, which is the *energy function*. Within the energy function, we have a **U**, which is the matrix of weights that we multiply our input by, and a **b** is the matrix of biases. We also have the restriction that **U** is a *positive definite* matrix, which means that it is symmetric and has all positive eigenvalues. This just ensures that our first matrix multiplication term, which is quadratic because we multiply by **x** *and* **x**<sup>T</sup>, will always dominate the linear second term. This needs to be the case because then the energy function will always be positive, and then the negative exponential of that positive value will converge and therefore integrate to 1, given a finite normalizing constant.

Google "exponential function" versus "negative exponential" and look at the images to quickly see why the integral of a positive exponential function is infinite while the integral of a negative exponential is finite and could be normalized to be 1.


If we expand the first matrix multiplication term,

$$
  \textbf{x}^T \textbf{U} \textbf{x} =
  \begin{bmatrix} x_1  &  x_2  &  \ldots  &  x_n \end{bmatrix}
  \Bigg[ \textbf{u}_1  \quad  \textbf{u}_2  \quad  \ldots  \quad  \textbf{u}_n \Bigg]
  \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix}

  = \begin{bmatrix} \textbf{x}^T \textbf{u}_1  &  \textbf{x}^T \textbf{u}_2  &  \ldots  &  \textbf{x}^T \textbf{u}_n \end{bmatrix}
  \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix}
$$

Which we observe is a scalar, since each **x**<sup>T</sup>**u**<sub>*i*</sub> is a scalar.

### Maximum likelihood

Before we move on, let's take a moment to understand how this relates to the maximum likelihood estimator from the previous post. A quick refresher: a likelihood function is simply the product of the conditional probabilities that we get each of our observations **x**<sub>i</sub> out of our (assumed) distribution given some parameter *theta*.

$$
  \prod^n_{i = 1} p(\textbf{x}_i \vert \theta) = L(\theta \vert \textbf{x}_1, \ldots, \textbf{x}_n)
$$

To say it another way, we *assume* that our distribution is of a particular type - in this case, a Boltzmann Distribution - and that it depends on some parameter *theta*. The likelihood that that theta is really what governs the distribution given the observations **x**<sub>i</sub> that we've seen is the product of the probabilities that we get out each observation **x**<sub>i</sub> given that the distribution *does really* depend on that theta.

In this case, with the Boltzmann Distribution, the *theta* is the weights matrix **U** and the biases matrix **b**.

Therefore, our maximum likelihood estimator is the max likelihood over all *theta* and the maximum likelihood estimate is the theta that gives the max likelihood.

$$
  \text{argmax}_{\theta \in \Theta} L(\theta \vert \textbf{x}_1, \ldots, \textbf{x}_n) = \hat{\theta}
$$

## RBMs

To formally define a **Restricted Boltzmann Machine** (referred to as a **RBM**), we need to make a couple things clear. So far, we've thought of the input to the energy function, the vector **x**, as our observations or samples from the distribution. RBMs switch that up a little - they assume that the input to the energy function **x** is composed of two parts, some number of *visible* variables, and some number of *hidden* variables:

$$
  \textbf{x} = (\textbf{v}, \textbf{h})
$$

We can then rewrite the energy function:

$$
  E(\textbf{v}, \textbf{h}) = \begin{bmatrix} \textbf{v}^T  &  \textbf{h}^T \end{bmatrix}
  \begin{bmatrix} \textbf{R}  &  \frac{1}{2}\textbf{W} \\ \frac{1}{2}\textbf{W}^T  &  \textbf{S} \end{bmatrix}
  \begin{bmatrix} \textbf{v} \\ \textbf{h} \end{bmatrix}

  + \begin{bmatrix} \textbf{b}^T \\ \textbf{c}^T \end{bmatrix}
  \begin{bmatrix} \textbf{v}  &  \textbf{h} \end{bmatrix}
$$

Note that we have decomposed **U** into four quarters, which are themselves matrices and which we compose out of matrices we name **R**, **W**, and **S**, and we have decomposed **b**<sup>T</sup> into **b**<sup>T</sup> and **c**<sup>T</sup>, which are the respective parts of the bias matrix that are multiplied by **v** and **h**. Because **U** is positive definite and therefore symmetric, the upper-right and lower-left quarters must be each other's transpose. We name them *1/2* **W** instead of just **W** for reasons that will become clear once we expand the first matrix multiplication:

$$
  \begin{bmatrix} \textbf{v}^T  &  \textbf{h}^T \end{bmatrix}
  \begin{bmatrix} \textbf{R}  &  \frac{1}{2}\textbf{W} \\ \frac{1}{2}\textbf{W}^T  &  \textbf{S} \end{bmatrix}
  \begin{bmatrix} \textbf{v} \\ \textbf{h} \end{bmatrix}
$$

$$
  = \begin{bmatrix} \textbf{v}^T \textbf{R} + \frac{1}{2} \textbf{h}^T \textbf{W}^T  &  \frac{1}{2} \textbf{v}^T \textbf{W} + \textbf{h}^T \textbf{S} \end{bmatrix}
  \begin{bmatrix} \textbf{v} \\ \textbf{h} \end{bmatrix}
$$

$$
  = \textbf{v}^T \textbf{R} \textbf{v} + \frac{1}{2} \textbf{h}^T \textbf{W}^T \textbf{v} + \frac{1}{2} \textbf{v}^T \textbf{W} \textbf{h} + \textbf{h}^T \textbf{S} \textbf{h}
$$

and by applying the property of matrix multiplication that (**AB**)<sup>T</sup> = **B**<sup>T</sup>**A**<sup>T</sup> on the second term, we have

$$
  \textbf{h}^T \textbf{W}^T \textbf{v} = (\textbf{W} \textbf{h})^T \textbf{v} = [\textbf{v}^T (\textbf{W} \textbf{h})]^T = \textbf{v}^T \textbf{W} \textbf{h}
$$

The last equality is because the triple matrix multiplication results in a scalar value and the transpose of a scalar value is the scalar value. Therefore,

$$
  E(\textbf{v}, \textbf{h})= \textbf{v}^T \textbf{R} \textbf{v} + \textbf{v}^T \textbf{W} \textbf{h} + \textbf{h}^T \textbf{S} \textbf{h} + \textbf{b}^T \textbf{v} + \textbf{c}^T \textbf{h}
$$

If we ignore the first term, which depends on the visible units and the upper-left quarter of the weights, and the third term, which depends on the hidden units and the lower-right quarter of the weights, we are left with the second term, which depends on both the visible and hidden units and the remaining two quarters of the weight matrix, as well as the two bias terms, giving us the modified energy function below.

$$
  E(\textbf{v}, \textbf{h})= \textbf{v}^T \textbf{W} \textbf{h} + \textbf{b}^T \textbf{v} + \textbf{c}^T \textbf{h}
$$

Now, we see that the energy function is the visible units times some weights times the hidden units plus some biases applied to both the visible and hidden units.

Where else have we seen this?

## Feedforward Neural Networks

![Feedforward neural network](/img/dl-part-2/feedforward.png "Feedforward neural network")

Thanks to [Evan Wallace's Finite State Machine Designer](http://madebyevan.com/fsm/).

We observe that in a feedforward neural network, the input - each visible unit - is multiplied by some weight, which is then fed into each hidden unit, which we see corresponds to the first matrix multiplication term of the energy function, **v**<sup>T</sup> **W** **h**. Biases are also applied in both the visible and hidden units, which correspond to the second and third bias terms in the energy function, **b**<sup>T</sup> **h** and **c**<sup>T</sup> **h**.

From before, we can write the *conditional distribution* of our visible (observed) units and our hidden units given some weights and biases *theta* as follows:

$$
  p(\textbf{v}, \textbf{h} \vert \theta) = \frac{1}{Z} \exp (-E(\textbf{v}, \textbf{h}))
$$

We can actually derive, using the laws of probability plus a bunch of matrix algebra, the conditional distributions for **v** and **h**:

$$
  p(h_j \vert \textbf{v}) = \sigma(\sum_i W_{ij} v_i + b_j)
$$

and

$$
  p(v_i \vert \textbf{h}) = \sigma(\sum_j W_{ij} h_j + b_i)
$$

See [Salakhutdinov's papers on RBMs](http://proceedings.mlr.press/v5/salakhutdinov09a/salakhutdinov09a.pdf) for more information and the derivation.

Why is this important? Feedforward neural networks often use the sigmoid function,

$$
  \sigma(x) = \frac{e^{-x}}{1 + e^{-x}}
$$

as the nonlinearity for activation of a unit. This connection between the matrix algebra mathematics and the derivation of the sigmoid function directly from Restricted Boltzmann Machines means that there is actually a mathematical basis for why neural networks take the form that they do, and it provides theoretical validity to the architecture of neural networks that we know. Most of these distributions ins tatistics and machine learning are taught because they *work* - the Boltzmann Distribution, for example, is notable because it does a good job of modeling natural phenomena. Many many distributions and methods are lost because, while mathematically novel, they aren't useful. The ones we do remember are the ones that model important things, or which can be used to solve interesting problems.

The link between RBMs and feedforward neural networks is that the math behind RBMs actually show the validity and viability of feedforward neural networks. The difference between RBMs and feedforward neural networks is that RBMs are a type of *probabilistic model* while feedforward neural networks are *deterministic*. We just take the mean of the first conditional distribution *p(h<sub>j</sub> \| **v**)* to get our deterministic neural networks.
