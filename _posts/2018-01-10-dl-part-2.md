---
layout: post
title:  "Deep Learning Part 2 - Restricted Boltzmann Machines and Neural Nets"
date:   2018-01-10 10:45:00 -0800
categories: blog dl tutorial
---
<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

This is the second in a several-part series on the basics of deep learning, presented in an easy-to-read, lightweight format. [Here]({% post_url 2017-12-30-dl-part-1 %}) is a link to the first one. Previous experience with basic probability and matrix algebra will be helpful, but not required. Send any comments or corrections to [josh@jzhanson.com](mailto:josh@jzhanson.com).

Mathematically, Restricted Boltzmann Machines are derived from the Maxwell-Boltzmann Distribution plus matrix algebra, which we'll go over in this post. We'll also use that as a bridge to connect to the basics of neural networks.

## The Boltzmann Distribution

Let us first define **x** to be a vector of *n* outcomes, where each *x<sub>i</sub>* can either be 0 or 1. Of course, each *x<sub>i</sub>* can have a different probability of being 1. The probabilities can even be conditional, *a la* Markov Chains. But more on that later. In the previous post, we have usually thought of *x* as being a single random variable. Here, however, it is a vector of individual random variables.

$$
  \textbf{x} = \begin{bmatrix} x_1  &  x_2  &  \ldots  &  x_n \end{bmatrix}, \: x_i \in \{0, 1\}
$$

With that definition out of the way, we can examine the Boltzmann distribution, invented by Ludwig Boltzmann, which models a bunch of things in physics, like how a hot object cools, or how energy dissipates into the environment. We have

$$
  P(x) = \frac{1}{Z} \exp (-E(\textbf{x})), \: E(\textbf{x}) = \textbf{x}^T \textbf{U} \textbf{x} + \textbf{b}^T \textbf{x}
$$

Here, *Z* is the partition function or normalizing constant which makes sure that the distribution integrates to one, which is just used to normalize the probabilities and make sure the distribution integrates to 1. The exp function is the same as raising the constant *e* to the function argument, which is the *energy function*. Within the energy function, we have a **U**, which is the matrix of weights that we multiply our input by, and a **b** is the matrix of biases. We also have the restriction that **U** is a *positive definite* matrix, which means that it is symmetric and has all positive eigenvalues. This just ensures that our first matrix multiplication term, which is quadratic because we multiply by **x** *and* **x**<sup>T</sup>, will always dominate the linear second term. This needs to be the case because then the energy function will always be positive, and then the negative exponential of that positive value will converge and therefore integrate to 1, given a finite normalizing constant.

Google "exponential function" versus "negative exponential" and look at the images to quickly see why the integral of a positive exponential function is infinite while the integral of a negative exponential is finite and could be normalized to be 1.


If we expand the first matrix multiplication term,

$$
  \textbf{x}^T \textbf{U} \textbf{x} =
  \begin{bmatrix} x_1  &  x_2  &  \ldots  &  x_n \end{bmatrix}
  \Bigg[ \textbf{u}_1  \quad  \textbf{u}_2  \quad  \ldots  \quad  \textbf{u}_n \Bigg]
  \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix}

  = \begin{bmatrix} \textbf{x}^T \textbf{u}_1  &  \textbf{x}^T \textbf{u}_2  &  \ldots  &  \textbf{x}^T \textbf{u}_n \end{bmatrix}
  \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix}
$$

Which we observe is a scalar, since each **x**<sup>T</sup>**u**<sub>*i*</sub> is a scalar.

### Maximum likelihood

Before we move on, let's take a moment to understand how this relates to the maximum likelihood estimator from the previous post. A quick refresher: a likelihood function is simply the product of the conditional probabilities that we get each of our observations **x**<sub>i</sub> out of our (assumed) distribution given some parameter *theta*.

$$
  \prod^n_{i = 1} p(\textbf{x}_i \vert \theta) = L(\theta \vert \textbf{x}_1, \ldots, \textbf{x}_n)
$$

To say it another way, we *assume* that our distribution is of a particular type - in this case, a Boltzmann Distribution - and that it depends on some parameter *theta*. The likelihood that that theta is really what governs the distribution given the observations **x**<sub>i</sub> that we've seen is the product of the probabilities that we get out each observation **x**<sub>i</sub> given that the distribution *does really* depend on that theta.

In this case, with the Boltzmann Distribution, the *theta* is the weights matrix **U** and the biases matrix **b**.

Therefore, our maximum likelihood estimator is the max likelihood over all *theta* and the maximum likelihood estimate is the theta that gives the max likelihood.

$$
  \text{argmax}_{\theta \in \Theta} L(\theta \vert \textbf{x}_1, \ldots, \textbf{x}_n) = \hat{\theta}
$$

## RBMs

To formally define a **Restricted Boltzmann Machine** (referred to as a **RBM**), we need to make a couple things clear. So far, we've thought of the input to the energy function, the vector **x**, as our observations or samples from the distribution. RBMs switch that up a little - they assume that the input to the energy function **x** is composed of two parts, some number of *visible* variables, and some number of *hidden* variables:

$$
  \textbf{x} = (\textbf{v}, \textbf{h})
$$

We can then rewrite the energy function:

$$
  E(\textbf{v}, \textbf{h}) = \begin{bmatrix} \textbf{v}^T  &  \textbf{h}^T \end{bmatrix}
  \begin{bmatrix} \textbf{R}  &  \frac{1}{2}\textbf{W} \\ \frac{1}{2}\textbf{W}^T  &  \textbf{S} \end{bmatrix}
  \begin{bmatrix} \textbf{v} \\ \textbf{h} \end{bmatrix}

  + \begin{bmatrix} \textbf{b}^T \\ \textbf{c}^T \end{bmatrix}
  \begin{bmatrix} \textbf{v}  &  \textbf{h} \end{bmatrix}
$$

Note that we have decomposed **U** into four quarters, which are themselves matrices and which we compose out of matrices we name **R**, **W**, and **S**, and we have decomposed **b**<sup>T</sup> into **b**<sup>T</sup> and **c**<sup>T</sup>, which are the respective parts of the bias matrix that are multiplied by **v** and **h**. Because **U** is positive definite and therefore symmetric, the upper-right and lower-left quarters must be each other's transpose. We name them *1/2* **W** instead of just **W** for reasons that will become clear once we expand the first matrix multiplication:

$$
  \begin{bmatrix} \textbf{v}^T  &  \textbf{h}^T \end{bmatrix}
  \begin{bmatrix} \textbf{R}  &  \frac{1}{2}\textbf{W} \\ \frac{1}{2}\textbf{W}^T  &  \textbf{S} \end{bmatrix}
  \begin{bmatrix} \textbf{v} \\ \textbf{h} \end{bmatrix}
$$

$$
  = \begin{bmatrix} \textbf{v}^T \textbf{R} + \frac{1}{2} \textbf{h}^T \textbf{W}^T  &  \frac{1}{2} \textbf{v}^T \textbf{W} + \textbf{h}^T \textbf{S} \end{bmatrix}
  \begin{bmatrix} \textbf{v} \\ \textbf{h} \end{bmatrix}
$$

$$
  = \textbf{v}^T \textbf{R} \textbf{v} + \frac{1}{2} \textbf{h}^T \textbf{W}^T \textbf{v} + \frac{1}{2} \textbf{v}^T \textbf{W} \textbf{h} + \textbf{h}^T \textbf{S} \textbf{h}
$$

and by applying the property of matrix multiplication that (**AB**)<sup>T</sup> = **B**<sup>T</sup>**A**<sup>T</sup> on the second term, we have

$$
  \textbf{h}^T \textbf{W}^T \textbf{v} = (\textbf{W} \textbf{h})^T \textbf{v} = [\textbf{v}^T (\textbf{W} \textbf{h})]^T = \textbf{v}^T \textbf{W} \textbf{h}
$$

The last equality is because the triple matrix multiplication results in a scalar value and the transpose of a scalar value is the scalar value. Therefore,

$$
  E(\textbf{v}, \textbf{h})= \textbf{v}^T \textbf{R} \textbf{v} + \textbf{v}^T \textbf{W} \textbf{h} + \textbf{h}^T \textbf{S} \textbf{h} + \textbf{b}^T \textbf{v} + \textbf{c}^T \textbf{h}
$$

If we ignore the first term, which depends on the visible units and the upper-left quarter of the weights, and the third term, which depends on the hidden units and the lower-right quarter of the weights, we are left with the second term, which depends on both the visible and hidden units and the remaining two quarters of the weight matrix, as well as the two bias terms, giving us the modified energy function below.

$$
  E(\textbf{v}, \textbf{h})= \textbf{v}^T \textbf{W} \textbf{h} + \textbf{b}^T \textbf{v} + \textbf{c}^T \textbf{h}
$$


