---
layout: post
title:  "Deep Learning Part 1 - Bayes' Rule and Boltzmann Machines"
date:   2017-12-26 10:25:00 -0800
categories: blog dl tutorial
---

This is the first in a several-part series on the basics of deep learning, presented in an easy-to-read, lightweight format. Previous experience with basic probability and matrix algebra will be helpful, but not required. Send any comments or corrections to [josh@jzhanson.com](mailto:josh@jzhanson.com).

## Bayes' Rule

We begin our discussion with **Bayes' rule**, an important result that captures the intuitive relationship between an event and prior knowledge we have of factors that might affect the probability of the event. Simply put, it formulates how event *B* affects the probability of event *A*. It forms the basis of Bayesian inference and Naive Bayes. Because it is a little difficult to grasp intuitively at first, let's go over its derivation from the definition of *conditional probability*, which is a little easier to understand intuitively.

### Conditional probability

Conditional probability simply formulates the probability of event *A* happening **given that** event *B* happened.

![Conditional probability](/img/dl-part-1/conditional-1.png "Basic conditional probability")

The *P*s basically mean "probability of," the vertical bar \| on the left side simply means "given," and the little upside-down u on the numerator of the right side means "and," as in event *A* happening *and* event *B* happening.

What conditional probability is saying is that the probability of event *A* given event *B* is equal to the probability of event *A* and event *B* happening divided by the probability of event *B*. It's a bit easier to see with a Venn diagram of probabilities.

![Conditional probability illustrated](/img/dl-part-1/conditional-2.png "Conditional probability illustrated")

It is fairly clear that if we assume that event *B* has happened and we wish to consider the probability of event *A* happening, then we only need to consider the probability space where *B* happens, that is, the right, darker circle *P(B)*. Within that circle, there's the darkest section, *P(A and B)*, which is how *A* can happen if we assume that *B* happened. So we can see that the probability of *A* given *B* is equal to the probability of *A* and *B* (how *A* can still happen given that *B* happened) divided by the total probability space under consideration, *P(B)*, because, again, we're assuming that *B* happened.

![Bayes' from conditional probability](/img/dl-part-1/conditional-3.png "Derivation of Bayes' Rule")

We first multiply the denominators on both formulas, set the two formulas equal, because "and" is communative - *A* and *B* happening is the same as *B* and *A* happening, and finally multiply *P(B)* over, assuming that that probability is not zero, we can easily derive Bayes' Rule.

### Generalizing Bayes' Rule

![Vanilla Bayes' Rule](/img/dl-part-1/bayes-rule-1.png "Straight out of probability 101")

To build some more intuition, we use the Law of Total Probability, which states the probability of any event *A* is equal to the probability of that event *A* happening given some event *B* happening times the probability that *B* happens, plus the probability of that event *A* happening given some event *B* happening times the probability *B* doesn't happen. To refer to the diagram above, we're basically saying that the probability of *A* is equal to the dark middle portion, *A* happening given *B* happening, plus the lightest shaded portion, *A* happening but *B* not happening. The bar above an event just means the complement of that event - i.e. the event of that event not happening.

![Law of Total Probability](/img/dl-part-1/total-prob.png "Law of Total Probability")

We rewrite Bayes' rule as follows using the Law of Total Probability, replacing the denominator:

![Bayes' Rule plus Law of Total Probability](/img/dl-part-1/bayes-total-prob.png "Law of Total Probability")

This is for the two variable case, but it is not difficult to see that it generalizes to any finite number of variables, say, if several outcomes *partition* the *probability space*. Those fancy words aren't super important - the takeaway is that we can write in the general case, with multiple events *B<sub>1</sub>*, *B<sub>2</sub>*, ..., *B<sub>n</sub>*, that



