---
layout: post
title:  "Deep Learning Part 1 - Bayes' Rule and Naive Bayes"
date:   2017-12-30 11:45:00 -0800
categories: blog dl tutorial
---

This is the first in a several-part series on the basics of deep learning, presented in an easy-to-read, lightweight format. Previous experience with basic probability and matrix algebra will be helpful, but not required. Send any comments or corrections to [josh@jzhanson.com](mailto:josh@jzhanson.com).

## Bayes' Rule

We begin our discussion with **Bayes' rule**, an important result that captures the intuitive relationship between an event and prior knowledge we have of factors that might affect the probability of the event. Simply put, it formulates how event *B* affects the probability of event *A*. It forms the basis of Bayesian inference and Naive Bayes. Because it is a little difficult to grasp intuitively at first, let's go over its derivation from the definition of *conditional probability*, which is easier to understand at first.

### Conditional probability

Conditional probability simply formulates the probability of event *A* happening **given that** event *B* happened.

![Conditional probability](/img/dl-part-1/conditional-1.png "Basic conditional probability")

The *P*s basically mean "probability of," the vertical bar \| on the left side simply means "given," and the little upside-down u on the numerator of the right side means "and," as in event *A* happening *and* event *B* happening.

What conditional probability is saying is that the probability of event *A* given event *B* is equal to the probability of event *A* and event *B* happening divided by the probability of event *B*. It's a bit easier to see with a Venn diagram of probabilities.

![Conditional probability illustrated](/img/dl-part-1/conditional-2.png "Conditional probability illustrated")

It is fairly clear that if we assume that event *B* happens and we wish to consider the probability of event *A* happening, then we only need to consider the probability space where *B* happens, that is, the right, darker circle *P(B)*. Within that circle, there's the darkest section, *P(A and B)*, which is how *A* can happen if we assume that *B* happens. So we can see that the probability of *A* given *B* is equal to the probability of *A* and *B* (how *A* can still happen given that *B* happens) divided by the total probability space under consideration, *P(B)*, because, again, we're assuming that *B* happes.

![Bayes' from conditional probability](/img/dl-part-1/conditional-3.png "Derivation of Bayes' Rule")

We first multiply the denominators on both formulas, set the two formulas equal, because "and" is communative - *A* and *B* happening is the same as *B* and *A* happening - and finally divide *P(B)* over, assuming that that probability is not zero, we can easily derive Bayes' Rule.

### Generalizing Bayes' Rule

![Vanilla Bayes' Rule](/img/dl-part-1/bayes-rule-1.png "Straight out of probability 101")

We use the Law of Total Probability, which states the probability of any event *A* is equal to the probability of that event *A* happening given some event *B* happening times the probability that *B* happens, plus the probability of that event *A* happening given some event *B* happening times the probability *B* doesn't happen. To refer to the diagram above, we're basically saying that the probability of *A* is equal to the dark middle portion, *A* happening given *B* happening, plus the lightest shaded portion, *A* happening but *B* not happening. The bar above an event just means the complement of that event - i.e. the event of that event not happening. For example, if I were flipping two coins, the probability that the second coin is heads is the probability the first coin is heads and the second coin is heads times the probability the first coin is heads, plus the probability that the first coin is not heads and the second coin is heads times the probability the the first coin is not heads.

![Law of Total Probability](/img/dl-part-1/total-prob.png "Law of Total Probability")

We rewrite Bayes' rule as follows using the Law of Total Probability, replacing the denominator:

![Bayes' Rule plus Law of Total Probability](/img/dl-part-1/bayes-total-prob.png "Law of Total Probability")

This is for the two variable case, but it is not difficult to see that it generalizes to any finite number of variables, say, if several outcomes *partition* the *sample space*, which means that exactly one of these events *must* happen. So, instead of just having two outcomes, *B* or *not B*, we have several. For example, the event of getting a one, a two, a three, a four, a five, or a six when rolling a dice are events that partition the sample space, because exactly one must happen when you roll the dice! The takeaway is that we can write in the general case, with multiple events *B<sub>1</sub>*, *B<sub>2</sub>*, ..., *B<sub>n</sub>*, that

![Bayes' Rule, general case](/img/dl-part-1/bayes-rule-general.png "Bayes' Rule, general case")

Now if we leave behind *discrete* probability and move to *continuous* probability, not too much changes besides we switch the summation to an integral and swap around some function notation, which we will introduce here. Note that the lowercase *p*s and *f*s mean more or less the same thing as the uppercase *P*s - they stand for the probability mass or probability density functions for discrete and continuous random variables, respectively. We usually use Greek letters, like *theta*, to stand for *hypotheses*, our guesses or estimates about the parameters of particular distributions. We will usually use little English letters, like *x*, to represent observations, or data values. Don't worry too much about whey there's a *p* here or an *f* there, it's just to make a distinction between *marginal* and *conditional* or *joint* distributions. Elsewhere, the notation may vary.

![Bayes' Rule, continuous](/img/dl-part-1/bayes-rule-2.png "Bayes' Rule, continuous")

In the context of machine learning, *x* is the *observation* - what we sample from some unknown distribution that we want to *model*, or *predict*. Theta is the unknown parameter that our unknown distribution depends upon, which is why we want to estimate or guess at what theta could be as best as we can in order to get a good prediction about the distribution and what data *x* we can sample from it. In fact, each term in the above equation has a name.

The numerator of the left side has *f(x \| theta)*, which we refer to as the *likelihood*, because it's the likelihood that we observe *x* if we fix some parameter value *theta*. We also have a *p(theta)*, which we call the *prior*, because it usually represents our prior knowledge of *theta* and how it's distributed - we have some prior knowledge of how theta behaves and which values it's likely to take. On the denominator of the right side, we have an integral over all values of *theta* of the likelihood times the prior, which we can see is just generalizing Bayes' Rule to the continuous case. We refer to this as the *evidence*, because it's what we know about the conditional distribution, *f(x \| theta)*, and the marginal, *p(theta)*, which is the *total probability*. Finally, we call the *p(theta \| x)* on the left side of the equation the *posterior* distribution, because it's the distribution we can infer after we combine the information we have from *likelihood* and the *prior* and apply Bayes' Rule. We can rewrite this, with words, as

![Bayes' Rule, with words](/img/dl-part-1/bayes-rule-3.png "Bayes' Rule, with words")

Note that because the denominator is constant with respect to theta and the sampled data *x<sub>1</sub>, ..., x<sub>n</sub>*, and the numerator is equal to the joint probability

![Bayes' Rule connection](/img/dl-part-1/bayes-rule-4.png "Bayes' Rule connection")

which is the probability that we see the sample data that we do given that paremeter *theta*. It is also worh noting here that I skimmed over the fact that our observed data *x* can be multiple sampled values, which are shown here both expanded and in the bolded vector form.

## Naive Bayes' Inference

### Chain rule for conditional probability

In a nutshell, the chain rule for conditional probability states that the probability of a bunch of things all happening is the probability of one of the things happening *given* the other things *happen* times the probability of all the other things happening.

![Chain rule of conditional probability](/img/dl-part-1/chain-rule-1.png "Chain rule of conditional probability")

The first line of the above is just to illustrate the change in notation, from the "cap" notation earlier to using commas to denote events both happening. We can repeatedly apply the chain rule, giving us

![Chain rule, repeated application](/img/dl-part-1/chain-rule-2.png "Chain rule, repeated application")

### Likelihood functions

Now if we sample, say, *n* samples from our unknown distribution, and the assumption here is that the samples are independent, then what we can do is if we know the likelihood function *f(x<sub>i</sub> \| theta)* and we want to find the probability that *theta* is a particular value given all our sampled data, we can repeatedly apply the chain rule of probabilities, replacing *p* with *f* since we are often dealing with continuous rather than discrete data:

![Basis of likelihood function](/img/dl-part-1/likelihood-1.png "Basis of likelihood function")

And finally, because we assume each *x<sub>i</sub>* is independent, we can drop all the other *x<sub>j</sub>* terms from each conditional probability distribution. This is because they're independent - i.e. the probability of *x<sub>i</sub>* being what it is does not at all depend on what value any other *x<sub>j</sub>* takes. This means that we have

![The likelihood function](/img/dl-part-1/likelihood-2.png "The likelihood function")

which we call the *likelihood* function. Note that because the *training data*, or *features*, we observed, *x<sub>1</sub>, ..., x<sub>n</sub>*, are fixed with respect to the likelihood function, the likelihood function is only a function of *theta*, the unknown parameter upon which our mystery distribution depends. In fact, it is exactly the probability that the parameter *theta* takes on whatever value it takes on. In other words, you can give me a value for *theta*, and I can use this likelihood function to tell you the probability that, given these training data *x<sub>1</sub>, ..., x<sub>n</sub>*, how likely it is that the real theta for our mystery distribution is the value that you gave me.

Note also that I don't require a concrete value for *theta* to construct the likelihood function - I only need some training data *x<sub>1</sub>, ..., x<sub>n</sub>*. So, if I wanted to model a particular, unknown, *black-box* distribution, I sample *n* samples from it, which I call my training data. I use this training data, Bayes' Rule, and the chain rule of conditional probability to construct my likelihood function. I then try to *maximize* that likelihood function. That is, I try to find the value of theta that gives me the highest likelihood that, given that my mystery distribution depends on the value of *theta* I picked, I would get out the training data that I did.

![Maximum likelihood](/img/dl-part-1/likelihood-3.png "Maximum likelihood")

We call the theta that gives us the highest probability from our likelihood function *theta-hat* - there exist more formal terms for it, but the carat that is used to signify a best-guess estimate looks like a hat. This is known as *maximum likelihood*, hence the subscript ML.

Now that I have an estimated distribution, I can ask the real mystery distribution for some more data samples, known as the *test data*. If, for each test data point, my estimated distribution says there's a high probability that I would get this particular point, then I say that my model *generalizes well*. If my estimated distribution has difficulty distinguishing this test data from, say, garbage data, then I say it *generalizes poorly*, perhaps suffering from *overfitting*. More on that in future posts.

I also used theta as a parameter rather than Wikipedia's *C<sub>k</sub>*, primarily because it is easier to grasp a single parameter rather than muliple class parameters. If we were to use the famous MNIST data set as an example, which deals with classifying handwritten digits, it is not hard to imagine that we would have a different parameter, modeling different data distributions, for each digit. Thus, we would be evaluating the maximum likelihood for which theta, that is, which digit's parameter, is most likely to give that particular test data point, and then we would say that the test data point would be the corresponding digit.

## Conclusion

To summarize, we began by explaiing how conditional probability is the basis of Bayes' Rule, how generalized Bayes' Rule plus the chain rule of conditional probability make a likelihood function, and how to use the likelihood function to guess a parameter of a mystery distribution, the basics of Naive Bayes inference and classification.

